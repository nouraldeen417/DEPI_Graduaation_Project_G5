Terraform-Based Infrastructure Provisioning for a Network Application
This document outlines the process of using Terraform to provision the infrastructure for a Python/Django network application on AWS. The setup includes modules for compute (EC2 instances), network (VPC and subnets), and security (security groups), with an S3 backend for state management. The Terraform configuration generates an Ansible inventory file for integration with the Ansible deployment .

1. Overview
The Terraform configuration provisions:

Network: A VPC with public and private subnets, an internet gateway, NAT gateway, and route tables.
Security: A security group allowing SSH (port 22) and HTTP (port 80) ingress, with open egress.
Compute: An EC2 instance in a public subnet to host the application.
Ansible Integration: Generates an AWS inventory file for Ansible to deploy the application using Docker Compose.

The configuration uses:

Modules: Reusable modules for network, security, and compute.
S3 Backend: Stores Terraform state in an S3 bucket (nour-tfstate).
Main File: Orchestrates module calls and inventory generation.
Provider: Configures AWS (with commented LocalStack settings for testing).


2. Prerequisites

Terraform: Installed on the control machine (e.g., brew install terraform or download from HashiCorp).
AWS Account: Configured with valid credentials (~/.aws/credentials) or IAM role.
AWS CLI: Optional, for verifying resources (aws configure).
S3 Bucket: Created for Terraform state (nour-tfstate in us-east-1).
Ansible Setup: The Ansible project structure from the ansible documentation, with the inventory/ directory for aws_inventory.yml.
Docker Images:
Application: nouraldeen152/networkapp:latest.
Nginx: nouraldeen152/nginx-reverse-proxy:latest.
Available in a registry (e.g., Docker Hub).


SSH Key Pair: An AWS key pair (my-key) for EC2 SSH access.
Project Structure:.
├── modules/
│   ├── compute/
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   ├── variables.tf
│   ├── network/
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   ├── variables.tf
│   ├── security/
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   ├── variables.tf
├── ansible/
│   ├── inventory/
│   │   ├── aws_inventory.yml
├── user.sh
├── main.tf


AMI: The specified AMI (ami-084568db4383264d4) must be valid in us-east-1.


3. Terraform Configuration
3.1. Main Configuration
File: main.tf
provider "aws" {
  region = "us-east-1"
}

terraform {
  backend "s3" {
    bucket = "nour-tfstate"
    key    = "terraform.tfstate"
    region = "us-east-1"
  }
}

module "network" {
  source = "./modules/network"
  vpc_name               = "main-vpc"
  vpc_cidr               = "10.0.0.0/16"
  vpc_reagion            = "us-east-1"
  public_subnet_count    = 2
  private_subnet_count   = 2
  availability_zones     = ["us-east-1a", "us-east-1b"]
  ngw_tagname            = "ngw"
  igw_tagname            = "igw"
  public_rtable_tagname  = "public-route-table"
  private_rtable_tagname = "private-route-table"
}

module "security" {
  source = "./modules/security"
  vpc_id  = module.network.vpc_id
  sg_name = "allow_ssh_http"
  ingress_rules = [
    {
      from_port   = 22
      to_port     = 22
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]  # Restrict in production
    },
    {
      from_port   = 80
      to_port     = 80
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
  ]
  egress_rules = [
    {
      from_port   = 0
      to_port     = 0
      protocol    = "-1"
      cidr_blocks = ["0.0.0.0/0"]
    }
  ]
}

module "compute" {
  source = "./modules/compute"
  ec2_instance = [
    {
      name               = "web-server"
      ami                = "ami-084568db4383264d4"
      instance_type      = "t2.micro"
      subnet_id          = module.network.public_subnet_ids[0]
      assign_public_ip   = true
      security_group_ids = [module.security.security_group_id]
      user_data          = file("${path.module}/user.sh")
      key_name           = "my-key"
    }
  ]
}

resource "local_file" "aws_inventory" {
  filename = "../ansible/inventory/aws_inventory.yml"
  content = join("\n", [
    "# AUTO-GENERATED BY TERRAFORM - DO NOT EDIT MANUALLY",
    "[aws_ec2]",
    join("\n", [
      for instance in module.compute.instance_public_ip :
      "${instance} ansible_user=management"
    ]),
    ""
  ])
  provisioner "local-exec" {
    when    = destroy
    command = "rm -f ../ansible/inventory/aws_inventory.yml"
  }
}

Explanation:

Provider: Configures AWS in us-east-1. Commented LocalStack settings allow local testing.
Backend: Stores state in the nour-tfstate S3 bucket.
Network Module: Creates a VPC, subnets, gateways, and route tables.
Security Module: Defines a security group for SSH and HTTP access.
Compute Module: Provisions an EC2 instance with user data for initialization.
Local File: Generates aws_inventory.yml for Ansible, cleaned up on terraform destroy.
Commented Resource: The null_resource for additional setup is commented out, indicating optional functionality.

3.2. Network Module
File: modules/network/main.tf
resource "aws_vpc" "main" {
  cidr_block = var.vpc_cidr
  tags = {
    Name = var.vpc_name
  }
}

resource "aws_subnet" "public" {
  count                   = var.public_subnet_count
  vpc_id                  = aws_vpc.main.id
  cidr_block              = cidrsubnet(var.vpc_cidr, 8, count.index)
  availability_zone       = var.availability_zones[count.index]
  map_public_ip_on_launch = true
  tags = {
    Name = "${var.vpc_name}-public-${count.index + 1}"
  }
}

resource "aws_subnet" "private" {
  count             = var.private_subnet_count
  vpc_id            = aws_vpc.main.id
  cidr_block        = cidrsubnet(var.vpc_cidr, 8, var.public_subnet_count + count.index)
  availability_zone = var.availability_zones[count.index]
  tags = {
    Name = "${var.vpc_name}-private-${count.index + 1}"
  }
}

resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id
  tags = {
    Name = var.igw_tagname
  }
}

resource "aws_nat_gateway" "main" {
  allocation_id = aws_eip.nat.id
  subnet_id     = aws_subnet.public[0].id
  tags = {
    Name = var.ngw_tagname
  }
}

resource "aws_eip" "nat" {}

resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id
  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }
  tags = {
    Name = var.public_rtable_tagname
  }
}

resource "aws_route_table_association" "public" {
  count          = var.public_subnet_count
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

resource "aws_route_table" "private" {
  vpc_id = aws_vpc.main.id
  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.main.id
  }
  tags = {
    Name = var.private_rtable_tagname
  }
}

resource "aws_route_table_association" "private" {
  count          = var.private_subnet_count
  subnet_id      = aws_subnet.private[count.index].id
  route_table_id = aws_route_table.private.id
}

Outputs: modules/network/outputs.tf
output "vpc_id" {
  value = aws_vpc.main.id
}

output "public_subnet_ids" {
  value = aws_subnet.public[*].id
}

output "private_subnet_ids" {
  value = aws_subnet.private[*].id
}

Variables: modules/network/variables.tf
variable "vpc_name" {}
variable "vpc_cidr" {}
variable "vpc_reagion" {}
variable "public_subnet_count" {}
variable "private_subnet_count" {}
variable "availability_zones" { type = list(string) }
variable "ngw_tagname" {}
variable "igw_tagname" {}
variable "public_rtable_tagname" {}
variable "private_rtable_tagname" {}

Explanation:

Creates a VPC (10.0.0.0/16) with 2 public and 2 private subnets across us-east-1a and us-east-1b.
Provisions an internet gateway for public subnets and a NAT gateway for private subnets.
Configures route tables for public (to internet gateway) and private (to NAT gateway) subnets.

3.3. Security Module
File: modules/security/main.tf
resource "aws_security_group" "main" {
  vpc_id = var.vpc_id
  name   = var.sg_name
  dynamic "ingress" {
    for_each = var.ingress_rules
    content {
      from_port   = ingress.value.from_port
      to_port     = ingress.value.to_port
      protocol    = ingress.value.protocol
      cidr_blocks = ingress.value.cidr_blocks
    }
  }
  dynamic "egress" {
    for_each = var.egress_rules
    content {
      from_port   = egress.value.from_port
      to_port     = egress.value.to_port
      protocol    = egress.value.protocol
      cidr_blocks = egress.value.cidr_blocks
    }
  }
  tags = {
    Name = var.sg_name
  }
}

Outputs: modules/security/outputs.tf
output "security_group_id" {
  value = aws_security_group.main.id
}

Variables: modules/security/variables.tf
variable "vpc_id" {}
variable "sg_name" {}
variable "ingress_rules" { type = list(any) }
variable "egress_rules" { type = list(any) }

Explanation:

Creates a security group allowing inbound SSH (port 22) and HTTP (port 80) from 0.0.0.0/0.
Allows all outbound traffic (protocol: -1).
Note: Restrict cidr_blocks in production (e.g., to specific IPs).

3.4. Compute Module
File: modules/compute/main.tf
resource "aws_instance" "main" {
  for_each               = { for idx, instance in var.ec2_instance : idx => instance }
  ami                    = each.value.ami
  instance_type          = each.value.instance_type
  subnet_id              = each.value.subnet_id
  associate_public_ip_address = each.value.assign_public_ip
  vpc_security_group_ids = each.value.security_group_ids
  user_data              = each.value.user_data
  key_name               = each.value.key_name
  tags = {
    Name = each.value.name
  }
}

Outputs: modules/compute/outputs.tf
output "instance_public_ip" {
  value = [for instance in aws_instance.main : instance.public_ip]
}

output "instance_ids" {
  value = [for instance in aws_instance.main : instance.id]
}

Variables: modules/compute/variables.tf
variable "ec2_instance" {
  type = list(object({
    name               = string
    ami                = string
    instance_type      = string
    subnet_id          = string
    assign_public_ip   = bool
    security_group_ids = list(string)
    user_data          = string
    key_name           = string
  }))
}

Explanation:

Provisions a t2.micro EC2 instance in a public subnet.
Uses the specified AMI and attaches the security group.
Applies user data from user.sh and assigns the my-key key pair.

3.5. User Data Script
File: user.sh
#!/bin/bash
# Example user data script (customize as needed)
apt-get update
apt-get install -y docker.io
systemctl enable docker
systemctl start docker
usermod -aG docker ubuntu

Explanation:

Installs Docker and adds the ubuntu user to the docker group.
Note: This script is minimal; the Ansible playbook provides more comprehensive setup.


4. Deployment Instructions

Prepare the Environment:

Install Terraform: brew install terraform.
Configure AWS credentials: aws configure or set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.
Create the S3 bucket nour-tfstate in us-east-1:aws s3 mb s3://nour-tfstate --region us-east-1




Set Up the Project:

Create the directory structure and place files as described.
Ensure the SSH key pair (my-key) exists in AWS us-east-1.
Verify the AMI (ami-084568db4383264d4) is valid.


Apply Terraform Configuration:
terraform init
terraform plan
terraform apply -auto-approve


Verify Resources:

Check EC2 instance:aws ec2 describe-instances --region us-east-1


Verify the Ansible inventory:cat ../ansible/inventory/aws_inventory.yml




Run Ansible Playbook:
cd ../ansible
ansible-playbook -i inventory/aws_inventory.yml site.yml -e "selected_roles=['docker_install','docker_compose'] IMAGE_TAG=20"


Access the Application:
curl http://<ec2-public-ip>:80  # Via Nginx
curl http://<ec2-public-ip>:8000  # Direct to Gunicorn


Clean Up:
terraform destroy -auto-approve



4. User Data Script for Management User Creation

File: user.sh

#!/bin/bash

# Create the management user with home directory and Bash shell
sudo useradd -m -s /bin/bash management

# Grant passwordless sudo privileges
sudo bash -c 'echo "management ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/management'
sudo chmod 440 /etc/sudoers.d/management

# Set up SSH directory and copy authorized_keys from the default user
sudo mkdir -p /home/management/.ssh
sudo cp "/home/ubuntu/.ssh/authorized_keys" /home/management/.ssh/authorized_keys

# Set ownership and permissions
sudo chown -R management:management /home/management/.ssh
sudo chmod 700 /home/management/.ssh
sudo chmod 600 /home/management/.ssh/authorized_keys
sudo echo "done creating management user with passwordless sudo and SSH access"

Usage in Terraform:

The script is referenced in the compute module via user_data = file("${path.module}/user.sh").
It is executed automatically during EC2 instance launch to configure the management user.

Explanation:

User Creation: Creates a management user with a home directory (/home/management) and Bash shell.

Sudo Privileges: Grants passwordless sudo access by adding a file to /etc/sudoers.d/.

SSH Access: Copies the SSH authorized_keys from the default ubuntu user (assumed for the AMI) to the management user's .ssh directory, enabling key-based SSH access.
Permissions: Sets secure ownership and permissions for the .ssh directory and authorized_keys file.
Integration with Ansible: The management user is used in the Ansible inventory (ansible_user=management) for deploying the application, ensuring consistency with SSH access.

5. Best Practices

Security:
Restrict security group ingress to specific IPs (e.g., your IP for SSH).
Use AWS Secrets Manager or Parameter Store for credentials.
Enable VPC flow logs for monitoring.


State Management:
Enable versioning on the nour-tfstate bucket.
Use remote state locking with DynamoDB.


Modularity:
Parameterize additional variables (e.g., region, instance type).
Add validation to module variables.


Ansible Integration:
Use Terraform's templatefile for more complex inventory formats.
Validate inventory generation with a linter.


Cost Management:
Use t3.micro instances for cost savings (if eligible for free tier).
Tag resources for cost allocation: aws:createdBy=terraform.




6. Troubleshooting

Terraform Init Errors:
Verify S3 bucket access: aws s3 ls s3://nour-tfstate.
Check AWS credentials: aws sts get-caller-identity.


EC2 Launch Failures:
Confirm AMI validity: aws ec2 describe-images --image-ids ami-084568db4383264d4.
Check subnet and security group attachments.


Ansible Inventory Issues:
Ensure ../ansible/inventory/ exists.
Verify public IP in aws_inventory.yml.


Application Access:
Check security group rules: aws ec2 describe-security-groups.
View EC2 logs: ssh -i my-key.pem ubuntu@<ec2-public-ip> 'docker logs <container>'.




7. Conclusion
This Terraform setup provisions a scalable AWS infrastructure for the Django network application, integrating seamlessly with Ansible for deployment. The modular design separates network, security, and compute concerns, while the S3 backend ensures reliable state management. The generated Ansible inventory enables automated application deployment using Docker Compose. For production, enhance security, state management, and cost optimization based on the best practices outlined.
For further details, refer to the official Terraform, AWS, Ansible, Docker, and Django documentation.
