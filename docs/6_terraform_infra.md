# Terraform-Based Infrastructure Provisioning for a Network Application

This document outlines the process of using Terraform to provision the infrastructure for a Python/Django network application on AWS. The setup includes modules for compute (EC2 instances), network (VPC and subnets), and security (security groups), with an S3 backend for state management. The Terraform configuration generates an Ansible inventory file for integration with the Ansible deployment described previously. A user data script creates a `management` user during EC2 instance launch. A diagram illustrates the Terraform design, showing AWS resources and their relationships. .

---

## 1. Overview

The Terraform configuration provisions:
1. **Network**: A VPC with public and private subnets, an internet gateway, NAT gateway, and route tables.
2. **Security**: A security group allowing SSH (port 22) and HTTP (port 80) ingress, with open egress.
3. **Compute**: An EC2 instance in a public subnet to host the application, with a user data script to create a `management` user.
4. **Ansible Integration**: Generates an AWS inventory file for Ansible to deploy the application using Docker Compose.

The configuration uses:
- **Modules**: Reusable modules for `network`, `security`, and `compute`.
- **S3 Backend**: Stores Terraform state in an S3 bucket (`nour-tfstate`).
- **Main File**: Orchestrates module calls and inventory generation.
- **Provider**: Configures AWS (with commented LocalStack settings for testing).
- **User Data**: A script to create a `management` user with passwordless sudo and SSH access.
- **Diagram**: A visual representation of the Terraform design in Mermaid syntax.

---

## 2. Prerequisites

- **Terraform**: Installed on the control machine (e.g., `brew install terraform` or download from HashiCorp).
- **AWS Account**: Configured with valid credentials (`~/.aws/credentials`) or IAM role.
- **AWS CLI**: Optional, for verifying resources (`aws configure`).
- **S3 Bucket**: Created for Terraform state (`nour-tfstate` in `us-east-1`).
- **Ansible Setup**: The Ansible project structure from the previous documentation, with the `inventory/` directory for `aws_inventory.yml`.
- **Docker Images**:
  - Application: `nouraldeen152/networkapp:20`.
  - Nginx: `nouraldeen152/nginx-reverse-proxy:1.0`.
  - Available in a registry (e.g., Docker Hub).
- **SSH Key Pair**: An AWS key pair (`my-key`) for EC2 SSH access.
- **Project Structure**:
  ```
  .
  ├── modules/
  │   ├── compute/
  │   │   ├── main.tf
  │   │   ├── outputs.tf
  │   │   ├── variables.tf
  │   ├── network/
  │   │   ├── main.tf
  │   │   ├── outputs.tf
  │   │   ├── variables.tf
  │   ├── security/
  │   │   ├── main.tf
  │   │   ├── outputs.tf
  │   │   ├── variables.tf
  ├── ansible/
  │   ├── inventory/
  │   │   ├── aws_inventory.yml
  ├── user.sh
  ├── main.tf
  ```
- **AMI**: The specified AMI (`ami-084568db4383264d4`) must be valid in `us-east-1`.

---

## 3. Terraform Configuration

### 3.1. Main Configuration

**File**: `main.tf`

```hcl
provider "aws" {
  region = "us-east-1"

}

terraform {
  backend "s3" {
    bucket = "xxxxxxx"
    key    = "xxxxxxxx"
    region = "xxxxxxx"
  }
}

module "network" {
  source = "./modules/network"
  vpc_name               = "main-vpc"
  vpc_cidr               = "10.0.0.0/16"
  vpc_reagion            = "us-east-1"
  public_subnet_count    = 2
  private_subnet_count   = 2
  availability_zones     = ["us-east-1a", "us-east-1b"]
  ngw_tagname            = "ngw"
  igw_tagname            = "igw"
  public_rtable_tagname  = "public-route-table"
  private_rtable_tagname = "private-route-table"
}

module "security" {
  source = "./modules/security"
  vpc_id  = module.network.vpc_id
  sg_name = "allow_ssh_http"
  ingress_rules = [
    {
      from_port   = 22
      to_port     = 22
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]  # Restrict in production
    },
    {
      from_port   = 80
      to_port     = 80
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
  ]
  egress_rules = [
    {
      from_port   = 0
      to_port     = 0
      protocol    = "-1"
      cidr_blocks = ["0.0.0.0/0"]
    }
  ]
}

module "compute" {
  source = "./modules/compute"
  ec2_instance = [
    {
      name               = "web-server"
      ami                = "ami-084568db4383264d4"
      instance_type      = "t2.micro"
      subnet_id          = module.network.public_subnet_ids[0]
      assign_public_ip   = true
      security_group_ids = [module.security.security_group_id]
      user_data          = file("${path.module}/user.sh")
      key_name           = "my-key"
    }
  ]
}

resource "local_file" "aws_inventory" {
  filename = "../ansible/inventory/aws_inventory.yml"
  content = join("\n", [
    "# AUTO-GENERATED BY TERRAFORM - DO NOT EDIT MANUALLY",
    "[aws_ec2]",
    join("\n", [
      for instance in module.compute.instance_public_ip :
      "${instance} ansible_user=management"
    ]),
    ""
  ])
  provisioner "local-exec" {
    when    = destroy
    command = "rm -f ../ansible/inventory/aws_inventory.yml"
  }
}
```

**Explanation**:
- **Provider**: Configures AWS in `us-east-1`. Commented LocalStack settings allow local testing.
- **Backend**: Stores state in the `nour-tfstate` S3 bucket.
- **Network Module**: Creates a VPC, subnets, gateways, and route tables.
- **Security Module**: Defines a security group for SSH and HTTP access.
- **Compute Module**: Provisions an EC2 instance with user data from `user.sh`.
- **Local File**: Generates `aws_inventory.yml` for Ansible, cleaned up on `terraform destroy`.

### 3.2. Network Module

**File**: `modules/network/main.tf`

```hcl
resource "aws_vpc" "main" {
  cidr_block = var.vpc_cidr
  tags = {
    Name = var.vpc_name
  }
}

resource "aws_subnet" "public" {
  count                   = var.public_subnet_count
  vpc_id                  = aws_vpc.main.id
  cidr_block              = cidrsubnet(var.vpc_cidr, 8, count.index)
  availability_zone       = var.availability_zones[count.index]
  map_public_ip_on_launch = true
  tags = {
    Name = "${var.vpc_name}-public-${count.index + 1}"
  }
}

resource "aws_subnet" "private" {
  count             = var.private_subnet_count
  vpc_id            = aws_vpc.main.id
  cidr_block        = cidrsubnet(var.vpc_cidr, 8, var.public_subnet_count + count.index)
  availability_zone = var.availability_zones[count.index]
  tags = {
    Name = "${var.vpc_name}-private-${count.index + 1}"
  }
}

resource "aws_internet_gateway" "main" {
  vpc_id =പ
resource "aws_vpc" "main" {
  cidr_block = var.vpc_cidr
  tags = {
    Name = var.vpc_name
  }
}

resource "aws_subnet" "public" {
  count                   = var.public_subnet_count
  vpc_id                  = aws_vpc.main.id
  cidr_block              = cidrsubnet(var.vpc_cidr, 8, count.index)
  availability_zone       = var.availability_zones[count.index]
  map_public_ip_on_launch = true
  tags = {
    Name = "${var.vpc_name}-public-${count.index + 1}"
  }
}

resource "aws_subnet" "private" {
  count             = var.private_subnet_count
  vpc_id            = aws_vpc.main.id
  cidr_block        = cidrsubnet(var.vpc_cidr, 8, var.public_subnet_count + count.index)
  availability_zone = var.availability_zones[count.index]
  tags = {
    Name = "${var.vpc_name}-private-${count.index + 1}"
  }
}

resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id
  tags = {
    Name = var.igw_tagname
  }
}

resource "aws_nat_gateway" "main" {
  allocation_id = aws_eip.nat.id
  subnet_id     = aws_subnet.public[0].id
  tags = {
    Name = var.ngw_tagname
  }
}

resource "aws_eip" "nat" {}

resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id
  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }
  tags = {
    Name = var.public_rtable_tagname
  }
}

resource "aws_route_table_association" "public" {
  count          = var.public_subnet_count
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

resource "aws_route_table" "private" {
  vpc_id = aws_vpc.main.id
  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.main.id
  }
  tags = {
    Name = var.private_rtable_tagname
  }
}

resource "aws_route_table_association" "private" {
  count          = var.private_subnet_count
  subnet_id      = aws_subnet.private[count.index].id
  route_table_id = aws_route_table.private.id
}
```

**Outputs**: `modules/network/outputs.tf`

```hcl
output "vpc_id" {
  value = aws_vpc.main.id
}

output "public_subnet_ids" {
  value = aws_subnet.public[*].id
}

output "private_subnet_ids" {
  value = aws_subnet.private[*].id
}
```

**Variables**: `modules/network/variables.tf`

```hcl
variable "vpc_name" {}
variable "vpc_cidr" {}
variable "vpc_reagion" {}
variable "public_subnet_count" {}
variable "private_subnet_count" {}
variable "availability_zones" { type = list(string) }
variable "ngw_tagname" {}
variable "igw_tagname" {}
variable "public_rtable_tagname" {}
variable "private_rtable_tagname" {}
```

**Explanation**:
- Creates a VPC (`10.0.0.0/16`) with 2 public and 2 private subnets across `us-east-1a` and `us-east-1b`.
- Provisions an internet gateway for public subnets and a NAT gateway for private subnets.
- Configures route tables for public (to internet gateway) and private (to NAT gateway) subnets.

### 3.3. Security Module

**File**: `modules/security/main.tf`

```hcl
resource "aws_security_group" "main" {
  vpc_id = var.vpc_id
  name   = var.sg_name
  dynamic "ingress" {
    for_each = var.ingress_rules
    content {
      from_port   = ingress.value.from_port
      to_port     = ingress.value.to_port
      protocol    = ingress.value.protocol
      cidr_blocks = ingress.value.cidr_blocks
    }
  }
  dynamic "egress" {
    for_each = var.egress_rules
    content {
      from_port   = egress.value.from_port
      to_port     = egress.value.to_port
      protocol    = egress.value.protocol
      cidr_blocks = egress.value.cidr_blocks
    }
  }
  tags = {
    Name = var.sg_name
  }
}
```

**Outputs**: `modules/security/outputs.tf`

```hcl
output "security_group_id" {
  value = aws_security_group.main.id
}
```

**Variables**: `modules/security/variables.tf`

```hcl
variable "vpc_id" {}
variable "sg_name" {}
variable "ingress_rules" { type = list(any) }
variable "egress_rules" { type = list(any) }
```

**Explanation**:
- Creates a security group allowing inbound SSH (port 22) and HTTP (port 80) from `0.0.0.0/0`.
- Allows all outbound traffic (`protocol: -1`).
- **Note**: Restrict `cidr_blocks` in production (e.g., to specific IPs).

### 3.4. Compute Module

**File**: `modules/compute/main.tf`

```hcl
resource "aws_instance" "main" {
  for_each               = { for idx, instance in var.ec2_instance : idx => instance }
  ami                    = each.value.ami
  instance_type          = each.value.instance_type
  subnet_id              = each.value.subnet_id
  associate_public_ip_address = each.value.assign_public_ip
  vpc_security_group_ids = each.value.security_group_ids
  user_data              = each.value.user_data
  key_name               = each.value.key_name
  tags = {
    Name = each.value.name
  }
}
```

**Outputs**: `modules/compute/outputs.tf`

```hcl
output "instance_public_ip" {
  value = [for instance in aws_instance.main : instance.public_ip]
}

output "instance_ids" {
  value = [for instance in aws_instance.main : instance.id]
}
```

**Variables**: `modules/compute/variables.tf`

```hcl
variable "ec2_instance" {
  type = list(object({
    name               = string
    ami                = string
    instance_type      = string
    subnet_id          = string
    assign_public_ip   = bool
    security_group_ids = list(string)
    user_data          = string
    key_name           = string
  }))
}
```

**Explanation**:
- Provisions a `t2.micro` EC2 instance in a public subnet.
- Uses the specified AMI and attaches the security group.
- Applies user data from `user.sh` and assigns the `my-key` key pair.

### 3.5. User Data Script for Management User Creation

**File**: `user.sh`

```bash
#!/bin/bash

# Create the management user with home directory and Bash shell
sudo useradd -m -s /bin/bash management

# Grant passwordless sudo privileges
sudo bash -c 'echo "management ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/management'
sudo chmod 440 /etc/sudoers.d/management

# Set up SSH directory and copy authorized_keys from the default user
sudo mkdir -p /home/management/.ssh
sudo cp "/home/ubuntu/.ssh/authorized_keys" /home/management/.ssh/authorized_keys

# Set ownership and permissions
sudo chown -R management:management /home/management/.ssh
sudo chmod 700 /home/management/.ssh
sudo chmod 600 /home/management/.ssh/authorized_keys
sudo echo "done creating management user with passwordless sudo and SSH access"
```

**Usage in Terraform**:
- The script is referenced in the `compute` module via `user_data = file("${path.module}/user.sh")`.
- It is executed automatically during EC2 instance launch to configure the `management` user.

**Explanation**:
- **User Creation**: Creates a `management` user with a home directory (`/home/management`) and Bash shell.
- **Sudo Privileges**: Grants passwordless sudo access by adding a file to `/etc/sudoers.d/`.
- **SSH Access**: Copies the SSH `authorized_keys` from the default `ubuntu` user (assumed for the AMI) to the `management` user's `.ssh` directory, enabling key-based SSH access.
- **Permissions**: Sets secure ownership and permissions for the `.ssh` directory and `authorized_keys` file.
- **Integration with Ansible**: The `management` user is used in the Ansible inventory (`ansible_user=management`) for deploying the application, ensuring consistency with SSH access.
- **Assumptions**:
  - The AMI uses `ubuntu` as the default user with an existing `.ssh/authorized_keys` file.
  - The AWS key pair (`my-key`) is associated with the instance, populating `authorized_keys`.
- **Customization**:
  - Modify the default user (`ubuntu`) if the AMI uses a different user (e.g., `ec2-user` for Amazon Linux).
  - Add error handling (e.g., check if `authorized_keys` exists) for robustness.
- **Security Note**: Passwordless sudo is convenient for automation but risky in production. Consider limiting sudo privileges or requiring a password.

---

## 4. Terraform Design Diagram

The following diagram illustrates the Terraform design for the network application infrastructure, including AWS resources and Ansible integration. It is provided in Mermaid syntax and can be rendered in tools like Mermaid Live Editor (https://mermaid.live/) to generate a visual image.


**Diagram Explanation**:
- **Terraform Configuration**: The root module (`main.tf`) orchestrates the provisioning of AWS infrastructure and generates the Ansible inventory.
- **AWS Infrastructure**:
  - **VPC**: `main-vpc` with CIDR `10.0.0.0/16`.
  - **Public Subnets**: Two subnets in `us-east-1a` and `us-east-1b`, connected to an internet gateway.
  - **Private Subnets**: Two subnets in `us-east-1a` and `us-east-1b`, connected to a NAT gateway.
  - **EC2 Instance**: A `t2.micro` instance (`web-server`) in a public subnet, running Docker containers (`networkapp:20`, `nginx-reverse-proxy:1.0`).
  - **Security Group**: Allows inbound SSH (port 22) and HTTP (port 80), attached to the EC2 instance.
  - **management User**: Created via user data script, enabling SSH and sudo access.
  - **S3 Bucket**: `nour-tfstate` stores Terraform state.
- **Ansible Integration**:
  - The Ansible inventory (`aws_inventory.yml`) is generated with the EC2 public IP and `management` user.
  - The Ansible playbook deploys the application to the EC2 instance.
- **Rendering Instructions**:
  - Copy the Mermaid code into Mermaid Live Editor (https://mermaid.live/).
  - Select "Flowchart" and render to view the diagram.
  - Export as PNG or SVG for use in documentation or presentations.
- **Notes**:
  - The diagram simplifies relationships (e.g., route tables are implied).
  - Customize node labels or styles in Mermaid for specific needs.

---

## 5. Deployment Instructions

1. **Prepare the Environment**:
   - Install Terraform: `brew install terraform`.
   - Configure AWS credentials: `aws configure` or set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
   - Create the S3 bucket `nour-tfstate` in `us-east-1`:
     ```bash
     aws s3 mb s3://nour-tfstate --region us-east-1
     ```

2. **Set Up the Project**:
   - Create the directory structure and place files as described.
   - Ensure the SSH key pair (`my-key`) exists in AWS `us-east-1`.
   - Verify the AMI (`ami-084568db4383264d4`) is valid.
   - Place the `user.sh` script in the project root.

3. **Apply Terraform Configuration**:
   ```bash
   terraform init
   terraform plan
   terraform apply -auto-approve
   ```

4. **Verify Resources**:
   - Check EC2 instance:
     ```bash
     aws ec2 describe-instances --region us-east-1
     ```
   - Verify the `management` user:
     ```bash
     ssh -i my-key.pem management@<ec2-public-ip> "whoami"
     ```
   - Verify the Ansible inventory:
     ```bash
     cat ../ansible/inventory/aws_inventory.yml
     ```

5. **Run Ansible Playbook**:
   ```bash
   cd ../ansible
   ansible-playbook -i inventory/aws_inventory.yml site.yml -e "selected_roles=['docker_install','docker_compose'] IMAGE_TAG=20"
   ```

6. **Access the Application**:
   ```bash
   curl http://<ec2-public-ip>:80  # Via Nginx
   curl http://<ec2-public-ip>:8000  # Direct to Gunicorn
   ```

7. **Clean Up**:
   ```bash
   terraform destroy -auto-approve
   ```

---

## 6. Best Practices

- **Security**:
  - Restrict security group ingress to specific IPs (e.g., your IP for SSH).
  - Use AWS Secrets Manager or Parameter Store for credentials.
  - Enable VPC flow logs for monitoring.
  - Limit `management` user sudo privileges in production.
- **State Management**:
  - Enable versioning on the `nour-tfstate` bucket.
  - Use remote state locking with DynamoDB.
- **Modularity**:
  - Parameterize additional variables (e.g., region, instance type).
  - Add validation to module variables.
- **Ansible Integration**:
  - Use Terraform's `templatefile` for more complex inventory formats.
  - Validate inventory generation with a linter.
- **Cost Management**:
  - Use `t3.micro` instances for cost savings (if eligible for free tier).
  - Tag resources for cost allocation: `aws:createdBy=terraform`.

---

## 7. Troubleshooting

- **Terraform Init Errors**:
  - Verify S3 bucket access: `aws s3 ls s3://nour-tfstate`.
  - Check AWS credentials: `aws sts get-caller-identity`.
- **EC2 Launch Failures**:
  - Confirm AMI validity: `aws ec2 describe-images --image-ids ami-084568db4383264d4`.
  - Check subnet and security group attachments.
- **User Data Issues**:
  - Verify `user.sh` execution: `ssh -i my-key.pem ubuntu@<ec2-public-ip> 'cat /var/log/cloud-init-output.log'`.
  - Ensure the default user (`ubuntu`) has `authorized_keys`.
- **Ansible Inventory Issues**:
  - Ensure `../ansible/inventory/` exists.
  - Verify `ansible_user=management` in `aws_inventory.yml`.
- **Application Access**:
  - Check security group rules: `aws ec2 describe-security-groups`.
  - View EC2 logs: `ssh -i my-key.pem management@<ec2-public-ip> 'docker logs <container>'`.

---

## 8. Conclusion

This Terraform setup provisions a scalable AWS infrastructure for the Django network application, integrating seamlessly with Ansible for deployment. The modular design separates network, security, and compute concerns, while the S3 backend ensures reliable state management. The `user.sh` script creates a `management` user during EC2 launch, enabling secure SSH access for Ansible. The generated Ansible inventory enables automated application deployment using Docker Compose. The Mermaid diagram provides a clear visual representation of the infrastructure. For production, enhance security, state management, and cost optimization based on the best practices outlined.

For further details, refer to the official Terraform, AWS, Ansible, Docker, and Django documentation.